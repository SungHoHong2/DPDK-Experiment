### [Supporting Fine-Grained Network Functions through Intel DPDK](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6984043)



- **NFV definition**
  - offers a new way to design, deploy and manage networking services
  - NFV decouples the network functions so it can run on software
    - network address translation (NAT)
    - firewalling
    - intrusion detection
    - domain name service (DNS)
    - caching
  - difference between SDN
    - NFV and SDN are both moves toward network virtualization and automation
    - but the goals are different
    - `SDN` can be considered a series of network objects such as switches, routers and firewalls
    - `NFV` is the process of moving services like load balancing, firewalls and intrusion prevention systems away from dedicated hardware into a virtualized environment.


- **cache line**
  - CPU caches transfer data from and to main memory in chunks called cache lines; a typical size for this seems to be 64 bytes.
  - Data that are located closer to each other than this may end up on the same cache line.
  - If these data are needed by different cores, the system has to work hard to keep the data consistent between the copies residing in the cores' caches. Essentially, while one thread modifies the data, the other thread is blocked by a lock from accessing the data.
  - By introducing padding into the structure to inflate it to 64 bytes, it is guaranteed that no two such data structures end up in the same cache line, and the processes that access them are not blocked more that absolutely necessary.

<br>

- **Problem**
  - Network Functions Virtualization (NFV) aims to transform network functions into software images, executed on standard, high-volume hardware.
  - focuses on the case in which a massive number of (tiny) network function instances are executed simultaneously on the same server
  - presents our experience in the design of the components that move the traffic across those functions


- **Solution**
  - proposes different possible architectures, it characterizes the resulting implementations,
  - it evaluates their applicability under different constraints.
  - In particular, efficient data exchange is one of the requirements mentioned in the deliverable
  - which provides the functional specification of a platform that is potentially able to deliver both computing and network virtualized services.
  - each one targeting a specific working condition, for transferring data between the virtual switch
  - Our goals include the necessity to scale with the number of NFs running on the server, which means that we should ensure high throughput and low latency even in case of a massive number of NFs operating concurrently, each one potentially traversed by a limited amount of traffic.

- **Conclusion**
  - Results obtained, particularly in terms of throughput, are quite satisfying for almost all the implementations proposed
  - goodness of the primitives exported by the DPDK;
  - only in few cases we spotted some limitations which are specific of our target domain
  - **In general, when the number of NF exceeded 100, the average latency experienced by the packets may become unacceptable in
real implementations.**
  - may not be satisfied with the current generation of the hardware, in which CPUs are dimensioned for a few, fat, jobs, while we have here many, tiny tasks.
  - This suggests that our future investigations should take into consideration different hardware platforms, such as the ones with a massive number of (tiny) cores, which may be more appropriate for our case

<br>

- **DPDK overview**
  - a software framework that offers to programmers a set of primitives that help to create efficient NFs on x86 platforms, in particular high speed data plane applications.
  - assumes that processes operate in polling mode
  - in order to be more efficient [3] and reduce the time spent by a packet traveling in the server
  - This would require each process to occupy one full CPU core
  - hence the number of processes running concurrently are limited by the CPU architecture
  - To manage memory, DPDK offers the rte_malloc and the `rte_mempool`
    - can be used to allocate objects on huge pages in order to reduce IOTLB misses
    - aligned with the cache line
    - on a particular NUMA socket in order to improve the performance
  - To exchange data among each others, lcores can use the `rte_ring`
    - a lockless FIFO queue that allows burst/bulk-single/multi-enqueue/dequeue operations.
    -  each slot of the buffer points to an `rte_mbuf`
      - `rte_mbuf` is an object in the rte_mempool that contains a pointer to the packet plus some additional metadata
  - `Poll Mode Driver (PMD) library` is the part of DPDK used by applications to access the network interface cards (NICs) without the intermediation (and the overhead) of the operating system.
    - `PMD` allows applications to exploit features offered by the Intel NIC controllers, such as RSS, FDIR, SR-IOV and VMDq.


<br>

- **architecture**
  - A virtual switch (vSwitch) module
  - receives packets from both NICs and NFs
  - classifies
  - delivers them to the proper NF according to the service chain each packet belongs to
  - vSwitch operates in polling mode as it is supposed to process a huge amount of traffic (each packet traverses the vSwitch multiple times)

- **implementation**
  - Unfortunately, vanilla DPDK does not support the execution of two different secondary processes on the same CPU core
    - we envision thousands of NFs deployed on the same physical server.
  - The multiproducer enqueue function implemented in DPDK does not allow two or more NFs executed on the same CPU core to use the same rte_ring
  - aims at reducing the load on the vSwitch, which is undoubtedly the most critical component of the system  
  - Each input queue is then associated with a different NF, while the output queue is just accessed by the vSwitch.
  - Since the number of hardware queues available on the NICs is limited, this architecture is appropriate when the number of NFs is reduced.


- **performance issues**
  - we experienced huge packet traveling times when the server was packed with many NF active at the same time.
  - In general, when the number of NF exceeded 100, the average latency experienced by the packets may become unacceptable in real implementations.
  - this suggests that our particular use case, with a massive number of (tiny) NFs, may not be satisfied with the current generation of the hardware, in which CPUs are dimensioned for a few, fat, jobs, while we have here many, tiny tasks


- **possible solutions**
  - find a different hardware that accepts tiny tasks
