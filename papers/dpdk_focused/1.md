### [Comparison of Frameworks for High-Performance Packet IO](http://delivery.acm.org/10.1145/2780000/2772729/p29-gallenmuller.pdf?ip=209.147.139.8&id=2772729&acc=ACTIVE%20SERVICE&key=B63ACEF81C6334F5%2EBD7B0059B564CDBA%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1519433659_aefe18d908154aa35dc3c2ef8adf6626)

- **abstract**
  - Network stacks currently implemented in operating systems can no longer cope with the packet rates offered by 10 Gbit Ethernet
  - analyze the performance of the most prominent frameworks based on representative measurements in packet forwarding scenarios
  - quantify the effects of caching and look at the tradeoff between throughput and latency
  - we introduce a model to estimate and assess the performance of these packet processing frameworks


- **problem**
  - 10 Gbit Ethernet adapters are commonly used in servers.
  - overhead imposed by the network stacksâ€™ architectural design the CPU quickly becomes the bottleneck
  - fix this issue by offering a stripped-down alternative to the Linux network stack.
  - allows using hardware systems as routers and (virtual) switches [4, 5], network middleboxes like firewalls [6], or network monitoring systems

- **hardware factors**
  - network bandwidth
  - CPU performance
  - PCI express bandwidth
  - connection
  - These factors are comprised into a model describing this type of program based on the previously mentioned frameworks
  - Various measurements show the applicability of this model, with packet forwarding as the basic test scenario
  - Each measurement is designed to investigate the influence of a specific factor on the forwarding throughput
    - the clock speed of the CPU
    - the number of processed packets per call
    - the cache utilization
    - latency of packet forwarding

  - the number of CPU cores has changed from one to a growing number of cores.
    - NICs need to support multi-core architectures explicitly by distributing incoming packets of different traffic flows to different cores
    - One of these techniques is Receive Side Scaling (RSS), which allows scaling packet processing with the number of cores

<br>

- **software factors**
  - NAPI, a network driver API introduced in kernel 2.4.20, reduces the number of interrupts generated by incoming traffic with the ability to switch to polling for packets during phases of high load
  - The NAPI-based network stack is sufficiently powerful to scale software routers to multiple Gbits
  - But even though performance improved, the Linux network stack primarily focuses on offering a fully featured general purpose network stack for an operating system (OS) rather than providing an interface for optimal performance needed for software router applications


<br>

- **high speed packet processing**
  - netmap, PF_RING, DPDK
  - Bypassing the default network stack
    - the packets are only processed by the processing framework and by the applications running on top of them.
  - Relying on polling to receive packets instead of interrupts
  - Preallocating packet buffers at the start of an application with no further allocation or deallocation of memory during execution of an application
  - No copying of data between user and kernel memory space as a packet is copied once to memory via DMA by the NIC and this memory location is used by processing framework and applications alike.
  - Processing batches of packets with one API call on reception and sending.


<br>


- **DPDK**
  - a collection of libraries, which offers not only basic functions for sending and receiving packets
  - provides additional functionality like a longest prefix matching algorithm for the implementation of routing tables and efficient hash maps
  - relies on a custom user space API similar to PF_RING ZC instead of traditional system calls used by netmap.
  - DPDK API [21] offers multicore support, additional libraries used for packet processing, and features the highest degree of configurability amongst the investigated frameworks.
  - UIO driver still has a part of its code realized as kernel module but its tasks are reduced
    - It only initializes the used PCI devices by mapping their memory regions into the user space process


- **model for performance of packet processing**
  - main factors influencing performance to provide an upper bound for the capabilities of a software-based packet processing system and to show the limits and potential bottlenecks.
    - The maximum transfer rate of the used `NICs`
    - `PCI express` is used to connect the NICs to the rest of the system
    - As packet data is sent to the `memory the RAM` could restrict the possible network bandwidth
      - If a CPU has to access a NIC or RAM attached to a different CPU
    - Due to modern offloading features of NICs the processing load on the `CPU` can be kept low.
      - if complex packet processing algorithms are performed, the CPU may lower the transfer rate even for high-speed packet
      frameworks.


<br>


- **the upperbound for packet processing**
- **High-Performance Prediction Model**


- **Performance Comparison**
  - available CPU cycles are the main limiting factor of software packet processing
  - throughput of a packet processing application heavily depends on the amount of CPU cycles available for its processing task
  - The overhead caused by
    - the complexity of packet processing
    - the time the CPU spends waiting for data to arrive in cache,
    - the effect of different batch sizes



- **Conclusion**
  - High-speed packet IO frameworks are no longer in fledgling stages and allow for a multiple of the packet rates of classical
network stacks.
  - The performance increase comes from processing in batches, preallocated buffers, and avoiding costly interrupts.
  - Starting with a model describing packet processing software in general, this model is gradually adapted to reflect applications using high-performance frameworks
  - can be applied to estimate processing tasks
  -
